<?xml version="1.0" encoding="UTF-8"?>
<contents>
    <content id="1">
        <title>Développement de la relativité</title>
        <image>relativite.jpg</image>
        <text><p>A la fin du 19ème siècle, plusieurs questions restent ouvertes pour les physiciens. Parmi ces problèmes se pose celui de la conciliation des équations de Maxwell avec la transformation de Galilée. En effet, les équations de Maxwell suggèrent que la lumière se propage à une vitesse $c$ constante. Or, selon la transformation de Galilée, la lumière ne peut se propager à $c$ dans tous les référentiels galiléens. On suppose alors à l'époque qu'il existe un référentiel particulier dans lequel les équations de Maxwell sont vérifiées, celui de l'"éther", considéré comme le support des ondes électromagnétiques. Si tout cela est correct, on devrait pouvoir mesurer la vitesse de la Terre par rapport à l'hypothétique éther en observant les déviations du comportement de la lumière par rapport à celui attendu dans le référentiel de l'éther. De nombreuses expériences en ce sens ont été effectuées, la plus célèbre étant sans doute celle de Michelson-Morley, effectuée à l'aide d'un interféromètre. Toutes ces expériences eurent un résultat négatif : la lumière se comportait comme prédit par les équations de Maxwell, même sur Terre, où sa vitesse de propagation semble donc également valoir $c$.</p>

            <p>En 1905, A. Einstein publie un article intitulé "Zur Elektrodynamik bewegter Körper" ("Sur l'électrodynamique des corps en mouvement") dans lequel il propose une théorie fondée sur le postulat selon lequel la lumière se propage à une même vitesse $c$ dans tous les référentiels galiléens. En d'autres mots, Einstein étend le principe de relativité selon lequel les lois de la Physique doivent être les mêmes dans tout référentiel Galiléen à l'électromagnétisme et donc au comportement de la lumière. Il en déduit qu'il faut abandonner la transformation de galilée au profit de la transformation de Lorentz et parvient à expliquer les observations faites concernant la propagation de la lumière avec succès. Il montre également comment la mécanique classique en est changée. </p>
            
        <quote author="Einstein" date="1905"><p>1. The laws by which the states of physical systems undergo change are not
            affected, whether these changes of state be referred to the one or the other of
            two systems of co-ordinates in uniform translatory motion.</p>
            <p>2. Any ray of light moves in the “stationary” system of co-ordinates with
                the determined velocity c, whether the ray be emitted by a stationary or by a
                moving body. </p>
        </quote>
        
            <p>Si la relativité restreinte décrit avec succès l'électrodynamique, elle souffre d'un inconvénient majeur : elle est incompatible avec la théorie de la gravitation de Newton (par exemple, l'action gravitationnelle d'une masse sur une autre est instantanée selon la vision Newtonienne alors que la relativité fait apparaitre une vitesse limite $c$ pour les interactions). Pourtant, celle-ci parait tout à fait correcte puisqu'elle semble expliquer tous les phénomènes gravitationnels en accord avec l'expérience. Einstein élabore donc une théorie "relativiste" de la gravitation, appelée relativité générale, qu'il met au point jusqu'en 1915. Cette théorie repose sur le principe d'équivalence : un champ de gravitation est équivalent à l'accélération d'un référentiel non inertiel par rapport à un référentiel inertiel.</p>
        
        TODO: (voir PM Amanuensis => de l'accélération au formalisme tensoriel).
        principe général, géométrie, équations einstein, succès expérimentaux.</text>
    </content>
    
    <content id="2">
        <title>Découverte de l'éloignement des galaxies</title>
        <image>redshift.jpg</image>
        <text><p>En 1912, Vesto Slipher mesure la vitesse de la galaxie Andromède par rapport à nous. Pour cela, il utilise la spectroscopie : en étudiant le spectre d'Andromède, il observe que certaines raies d'émission associées à des atomes bien connus sont légèrement déplacées. C'est le cas par exemple de la raie $H\alpha$ qui correspond à une transition électronique particulière dans l'atome d'hydrogène, qui émet à une longueur d'onde de 656,3 nm mais qui est observée à une longeur d'onde légèrement inférieure (un décalage d'un millième en valeur relative !). V. Slipher pense que cet écart est du à l'effet Doppler : une onde émise par une source en mouvement est perçue à une longueur d'onde différente de la longueur d'onde d'émission. Il connait aussi la relation entre la vitesse radiale d'éloignement de la source et la différence de longueur d'onde :
        \begin{equation}
        v_{radiale} = \dfrac{\lambda_{rec} - \lambda_0}{\lambda_0} c
        \end{equation}
        
        Pour Andromède, il trouve une valeur d'environ -300 km/s ! Le signe moins indique qu'elle se rapproche de nous. Cette valeur est un peu surprenante : elle est un ordre de grandeur supérieur à la vitesse typique des étoiles et des nébuleuses planétaires dont on avait mesuré les spectres<note>ceci est un exemple de note</note>. Cela a une conséquence importante, puisque cette vitesse exclut que les galaxies soient fortement liées gravitationnellement à la nôtre. On comprend alors à ce moment que cela favorise l'hypothèse selon laquelle ces objets sont relativement indépendants et de même nature que la Voie Lactée. Il n'était en effet pas exclu à l'époque que ces galaxies qu'on appelait alors "spirales nébuleuses" ne fassent partie de la Voie Lactée. </p>
        
        <p>Slipher continue ses observations sur les galaxies. En 1917, il a observé le spectre de 25 d'entre elles. Le résultat est étonnant : seules 4 parmi les 25 se rapprochent de nous. Toutes les autres s'éloignent ! Puisque la plupart des galaxies s'éloignent, les raies lumineuses (donc entre le rouge et le bleu) qu'elles émettent sont décalées vers des longueurs d'ondes plus grandes c'est-à-dire vers le rouge : on parle de décalage vers le rouge ou encore de redshift. Slipher trouve des vitesses dépassant 1000 km/s : il est clair que ces "spirales nébuleuses" sont des objets indépendants de notre galaxie, la Voie Lactée.</p></text>
    </content>
    
    <content id="3">
        <title>Débuts de la Cosmologie relativiste</title>
        <image>friedmann.jpg</image>
        <text><p>Une première application de la théorie de la relativité à la cosmologie est due à Einstein lui-même, en 1917. Il suppose que l'Univers respecte le principe cosmologique, c'est-à-dire qu'il est homogène et isotrope. Il suppose de plus que celui-ci est statique, et qu'il ne contient que de la matière non relativiste. Il réalise que pour concilier ces hypothèses, il faut introduire un nouveau terme dans l'équation d'Einstein : c'est ainsi qu'il ajoute à son modèle la constante cosmologique. Il trouve par ailleurs qu'un tel Univers doit avoir une courbure positive, c'est-à-dire une géométrie sphérique. Ce modèle, appelé Univers d'Einstein, présente quelques problèmes : d'abord, il est instable. D'autre part, la constante cosmologique, introduite comme paramètre, doit prendre une valeur très précise pour que l'Univers demeure statique.</p>
        <p>A cette époque, Einstein correspond avec Willem de Sitter, un physicien néerlandais. Celui-ci propose une alternative à l'Univers d'Einstein en requérant une certaine symétrie entre toutes les coordonnées de l'espace-temps, y compris le temps. (TODO: expliquer, voir pdf de sitter einstein, de sitter space = 4-sphere avec imaginary t-coordinate). Il remarque qu'un tel Univers est une solution du vide - c'est-à-dire en l'absence de toute forme de matière ou d'énergie - des équations d'Einstein avec une constante cosmologique quelconque. Selon la valeur de cette constante, un tel univers peut être en expansion ou au contraire en contraction.</p>
        <p>Une alternative à ces deux modèles est présentée quelques années plus tard par Alexandre Friedmann, un physicien et mathématicien russe. En 1922, il publie un article intitulé "Sur la courbature de l'espace", dans lequel il applique l'équation d'Einstein avec une constante cosmologique de valeur quelconque à un Univers homogène, isotrope, de géométrie sphérique ou plate et constitué de matière non relativiste. Cependant, à la différence d'Einstein, il ne le suppose pas statique. Il obtient alors ce qu'on appelle aujourd'hui les équations de Friedmann et trouve que l'Univers peut évoluer de plusieurs façons, comme s'expandre indéfiniment, ou observer une dynamique périodique. En 1924, il montre qu'il existe des solutions de géométrie hyperbolique.
        Ces résultats purement théoriques - Friedmann ne suggérant aucune expérience ou observation permettant de les confronter n'auront pas d'impact immédiat (TODO rephrase). </p>
        <p>Il faut attendre les travaux de Georges Lemaitre pour qu'un lien soit établi entre modèle cosmologique et observations astronomiques. En 1927, il propose un modèle qu'il appelle "Univers d'Einstein à rayon variable" (sphérique), c'est-à-dire similaire à celui décrit par Friedmann en 1922, mais avec quelques avancées notoires. En effet, en plus de la présence de matière non relativiste et de l'effet d'une constante cosmologique, Lemaitre intègre une composante ultrarelativiste de matière (rayonnement) à ses calculs. Mais surtout, il donne des arguments physiques en faveur de son modèle d'Univers variable. Premièrement, il note que le "modèle A" d'Einstein est pertinent puisqu'il tient compte de la présence de masse dans l'Univers. Mais il montre aussi un résultat très important : un Univers en expansion (comme dans le "modèle B" de De Sitter) peut expliquer la fuite apparente des "nébuleuses spirales" observée par Slipher ! Il est alors naturel de proposer un modèle d'Univers fait de matière et en expansion. Lemaitre obtient par ailleurs une relation liant la vitesse apparente de fuite $v$ d'une nébuleuse spirale telle que mesurée par effet doppler, la distance $d$ qui nous sépare de celle ci et le taux d'expansion de l'Univers (de rayon $R$) :
        \begin{equation}
        v = \left (\dfrac{c}{R} \dfrac{dR}{dt} \right ) d \textrm{ si } \ v \ll c \textrm{ càd } \ \ d \ll R
        \end{equation}
        Ce résultat peut être testé expérimentalement : il suffit de vérifier que la vitesse de fuite de galaxies est proportionnelle à leur distance avec nous. La mesure du coefficient de proportionnalité donne directement la valeur de $K = \frac{c\dot{R}}{R}$. La difficulté est d'évaluer ces distances.</p></text>
    </content>
    
    <content id="4">
        <title>Découverte de l'expansion de l'Univers</title>
        <image>hubble.jpg</image>
        <text><p>D'après les mesures de vitesse des nébuleuses spirales dues à Slipher, ces objets ne semblent pas appartenir à la Voie Lactée : ils se déplacent trop vite par rapport à elle pour y être liées gravitationnellement. Il est donc naturel pour mettre fin à ce débat de se demander à quelle distance celles-ci se trouvent. Les mesures de distance classiques comme la parallaxe ne s'appliquent pas à des objets si lointains : il faut trouver une autre méthode.</p>
        <p>La solution au problème de mesure de distance de ces objets lointains sera apportée par l'étude des céphéides. Les céphéides sont des étoiles variables périodiques : la puissance lumineuse qu'elles rayonnent varie avec une période $T$ de l'ordre de grandeur du jour. En 1908, l'astronome Henrietta Leavitt découvre une relation entre la luminosité de ces étoiles et leur période. Elle fait cette découverte à partir d'observations réalisées à l'observatoire de l'université d'Harvard sur des milliers d'étoiles variables pulsantes appartenant aux nuages de Magellan (des galaxies naines environ 20 fois plus proches de la Voie Lactée qu'Andromède). Ce résultat est très important : il permet de calculer la luminosité d'une céphéide à partir de sa seule période (qui est facilement mesurable). Or, connaissant la luminosité intrinsèque $L$ d'une étoile ainsi que le flux que l'on en reçoit par unité de surface sur Terre $F$ on peut en déduire sa distance $d$. ($F \propto L/d^2$). TODO:(optionnel car rallonge) expliquer le travail de Hertzsprung et Harlow Shapley qui a calibré la relation de Leavitt puisque celle-ci faisait intervenir la magnitude apparente et non absolue ne connaissant pas la distance des nuages de magellan)</p>
        <p>Edwin Hubble, un physicien américain, comprend très vite l'intérêt de cette méthode d'évaluation des distances. Durant les années 20, il applique cette méthode d'observation à des nébuleuses spirales suffisamment proches pour identifier individuellement des céphéides et appliquer la relation luminosité-distance alors connue. Connaissant la distance, il peut calculer la luminosité intrinsèque des plus brillantes des étoiles de ces nébuleuses. Il fit l'hypothèse que cette luminosité maximale devait être la même dans toutes les autres plus éloignées pour lesquelles il était impossible d'identifier les céphéides de façon individuelle. Ce faisant il disposait d'une nouvelle référence (la luminosité absolue des étoiles les plus brillantes) pour calculer la distance de chaque nébuleuse. En 1924, il annonce ainsi qu'il estime la distance d'Andromède à 900 000 années-lumière. Ce résultat met fin à la question du "grand débat" : les spirales nébuleuses sont bien des galaxies au même titre que la Voie Lactée à laquelle elles n'appartiennent pas.</p>
        <p>Dans son papier de 1927, Lemaitre propose un modèle de l'Univers dans lequel les galaxies environnantes peuvent paraitre s'éloigner avec une vitesse proportionnelle à leur distance du fait d'une expansion. A l'aide des premiers résultats combinés de mesures de distances d'Hubble et de vitesses radiales il établit même une estimation la valeur du coefficient de proportionnalité : $v = Kd$ et $K \sim $ 625 km/s/Mpc, mais les données lui manquent alors pour établir qu'il y a bien proportionnalité. Cette publication passe inaperçue*.</p>
        <p>En 1929, Hubble publie "A relation between distance and radial velocity among extra-galactic nebulae" (Une relation entre la distance et la vitesse radiale des nébuleuses extra-galactiques). Son article montre à partir de mesures de distances et vitesses radiales portant sur 46 nébuleuses qu'il existe une proportionnalité entre les deux. Hubble trouve donc $v = Kd$ où il estime la valeur de $K$ à 530 km/s/Mpc. Ce résultat, aujourd'hui appelé "Loi de Hubble", constitue la preuve de l'expansion de l'Univers, même aux yeux d'Einstein qui renonce alors à son modèle statique. La constante $K$ est aujourd'hui appelée "constante de Hubble" et notée $H_0$ ("H" pour Hubble, et "0" pour faire indiquer qu'il s'agit de la valeur de la constante au temps présent).
        
            <figure src="hubble_law.png" title="Figure issue du papier d'Hubble en 1929.">Légende originale traduite : <b>Relation vitesse-distance pour les nébuleuses extra-galactiques</b>. 
                Les vitesses radiales, corrigées du mouvement du Soleil, sont représentées en fonction des distances estimées à partir des étoiles contenues et des luminosités moyennes des nébuleuses d'un amas. Les disques noirs et le trait plein représentent la solution pour un mouvement solaire estimé en se basant sur les données individuelles des nébuleuses ; les cercles et la ligne pointillée représentent la solution obtenue et regroupant les nébuleuses en 9 groupes distincts ; la croix représente la vitesse moyenne et la distance moyenne de 22 nébuleuses dont les distances n'ont pu être estimées inidividuellement. </figure></p></text>
    </content>
    
    <content id="5">
        <title>Nouveaux modèles cosmologique : Big Bang ou Univers éternel ?</title>
        <image>bigbang.jpg</image>
        <text><p>Après la découverte de l'expansion de l'Univers par Hubble, Einstein, qui était jusqu'alors sceptique au sujet des travaux de Friedmann et Lemaitre sur des modèles cosmologiques non statiques, comprend leur valeur. Ainsi, au début des années 1930, il aide à répandre ces idées dans parmi les physiciens. En 1932, lui-même et De Sitter proposent un modèle cosmologique minimal, auquel on réfère aujourd'hui par le nom d'espace d'Einstein-de Sitter, conforme aux observations de l'époque : 
            <ul>
                <li>Géométrie plate</li>
                <li>Uniquement constitué de matière non-relativiste, de pression nulle (pas de rayonnement)</li>
                <li>Sans constante cosmologique</li>
            </ul>
            Il s'agit donc d'un Univers de Lemaitre-Friedmann à constante cosmologique nulle. 
            Ce modèle permet de déduire la densité de matière dans l'Univers directement à partir de la constante de Hubble $H_0$. On trouve ainsi avec les données de l'époque une densité de $10^{-25} \mbox{ kg.m}^{-3}$. Or il se trouve qu'il s'agit de l'ordre de grandeur de la densité telle qu'évaluée à partir des estimations de distances et masses des galaxies.
            Un élément majeur de ce modèle est qu'il implique l'apparition d'une singularité initiale : l'Univers semble naitre d'un état de densité infinie (facteur d'échelle nul), et ce il y a un peu plus d'un milliard d'années.
            Lemaitre qui avait déjà remarqué ce fait suggère en 1931 une explication. Il propose que l'Univers soit né de la désintégration d'un "atome", un état lié de la matière qui en se pulvérisant aurait engendré l'expansion. Il considère que ceci donne une explication aux rayons cosmiques et que la présence d'autres particules parmi ce rayonnement (alors non prouvée) en accréditerait la vraisemblance. Ainsi, pour Lemaitre, cette singularité est tout à fait physique.
            </p> 
            <p>
                En 1948, F. Hoyle pointe quelques problèmes qui suggèrent le besoin de formuler une autre théorie pour l'Univers :
                <ul>
                    <li>Problème de l'Âge de l'Univers : puisque le modèle d'Einstein-de Sitter implique que l'Univers soit né d'une singularité il entraine que celui-ci a un certain âge et que ses structures doivent être plus jeunes : dans ce cadre, et d'après la constante d'Hubble mesurée à l'époque, cet âge doit être d'un peu plus d'1 milliard d'années. Cependant, l'âge de la Terre était estimé à l'époque entre 1.5 et 3 milliards d'années (par des techniques radiométriques). </li>
                    <li>Problème de la formation des galaxies : selon Hoyle, le modèle statique d'Einstein peut expliquer la formation de condensations de matière (donc formation de galaxies) puisqu'il est instable aux contractions (une petite augmentation perturbative de la densité entrainerait une condensation encore plus forte). En revanche selon lui, [...]TODO. Note : univers de hoyle = localement univers d'einstein dans lequel une petite contraction est instable et poursuit : condensation ! MAIS condensations anciennesrequires par univers ES/FL</li>
                    <li></li>
                </ul>
                Hoyle propose alors un modèle d'univers appelé "théorie de l'état stationnaire" visant à résoudre ces problèmes. Dans sa théorie, il fait l'hypothèse que de la matière est créée continuument et de façon homogène - par exemple, sous forme d'atomes d'hydrogène - de sorte à ce que malgré l'expansion la densité d'énergie demeure constante. L'univers étant alors toujours de même densité, il est toujours semblable et n'a plus d'âge. TODO de plus localement Ein. Stat. donc condensations possible ? yes/no ?
            </p>
            <p>
                En 1950 on peut donc considérer qu'il existe deux classes de théories principales :
                <ul>
                    <li>Les univers d'Einstein-de Sitter et Friedmann-Lemaitre, avec singularité initiale et âge fini.</li>
                    <li>L'Univers stationnaire de Hoyle</li>
                </ul>
                Hoyle critiquera également la théorie de Lemaitre de l'atôme primitif et l'idée d'un état initial très dense de l'Univers en général par des arguments notamment philosophiques : il apparente la théorie de Lemaitre - qui est par ailleurs prêtre - à la Création biblique. Il fera référence à ce modèle qu'il conteste sous le nom de "Big Bang". C'est le premier emploi de cette dénomination dans la cosmologie. Les observations disponibles à l'époque ne permettant pas d'éliminer l'une de ces théories, 
            </p>
        </text>
    </content>
    
    
    <content id="6">
        <title>Premières indications matière noire</title>
        <image>dm.jpg</image>
        <text>
            <p>oort etc.</p>
            <p>TODO: mettre le truc classique avec l'application du viriel en ressource.</p>
        </text>
    </content>
    
    <content id="7">
        <title>Les débuts de la nucléosynthèse primordiale</title>
        <image>nucleosynthese.jpg</image>
        <text>
            <h3>La synthèse des éléments</h3>
            <p>Introduire un peu formation des éléments etc. + parler de hoyle </p>
            <p>Au début des années 1940, une hypothèse à l'étude est l'abondance relative des atomes dans l'Univers s'explique par un équilibre thermique rapide ayant eu lieu à une température $T$ qui aurait gelé les proportions des différentes espèces. Très approximativement, ces proportions devraient suivre une distribution de type Maxwell-Boltzmann $n \propto e^{-E/(k_B T)}$ où $E$ est leur énergie nucléaire de liaison. L'énergie de liaison augmentant linéairement avec la masse atomique, l'abondance des espèces devrait décroitre exponentiellement avec celle-ci. Mais ce n'est pas ce qu'on observe : au lieu de cela, l'abondance des espèces lourdes est à peu près constante. L'idée d'un équilibre thermique rapide est donc rejetée.
                <spoiler><figure src="abondance_equilibre_thermique.svg" title="logarithme de l'abondance relative des éléments et fit pour une distribution de boltzmann selon les énergies de liaison"> Le fit est réalisé sur la portion $0 \leq A \leq 80$ de la courbe La meilleure correspondance est atteinte pour une température d'équilibre de l'ordre de $10^{12}$ K, mais cela est incohérent avec le résultat pour des valeurs de $A$ supérieures (à partir d'environ 100-120) où la pente s'annulle inexplicablement.</figure></spoiler></p>
            <p>A partir de 1946, Gamow propose, en réponse à l'échec de cette explication, une autre théorie de formation des éléments basée sur un processus hors équilibre qu'il justifie par l'expansion rapide de l'Univers. Il montre dans le cadre du modèle d'Einstein-de Sitter que l'Univers se serait trouvé dans un état de densité suffisant pour autoriser des réactions nucléaires pendant un temps très court, vers ses tous premiers instants, pendant lequel un équilibre n'aurait pu être atteint. Gamow suggère alors un mécanisme, après qu'il ait remarqué une forte corrélation entre les sections efficaces de capture de neutrons par des noyaux et leur abondance :
            <ol>
               <li>Dans ses premiers instants, l'Univers se trouve dans un état où il est dominé par des neutrons</li> 
               <li>Les neutrons s'agglomèrent très vite par capture neutronique pour former successivement des éléments contenant de plus en plus de nucléons. Ceci doit se faire très rapidement étant donné le temps de demie-vie du neutron qui se désintègre en environ 1000 s : sinon, tous les neutrons seraient devenus des protons avant de s'agglomérer.</li> 
               <li>Ces éléments lourds qui se forment se stabilisent par radioactivité $\beta^-$ (leurs neutrons deviennent des protons) donnant les atomes stables dont on mesure aujourd'hui l'abondance.</li>
               <li>Les neutrons finissent par se désintégrer, et par ailleurs l'expansion ralentit la chaine d'agglomérations.</li>
            </ol>
            
                <figure src="neutron_capture_abundance.png" title="Corrélation entre section efficace de capture neutronique et abondance">Ce graphe tiré de "The theory of origin and relative abundances distribution of the elements"<note>Alpher et Herman, Physical Review 22.153</note> montre la corrélation entre abondance relative d'une noyau $_{Z}^{A}X$ et la section efficace de capture neutronique $_{Z}^{A}X+n \rightarrow _{Z}^{A+1}X$. Ceci montre que les éléments les moins abondants sont les plus susceptibles de capturer un neutron, ce qui suggère un mécanisme comme celui proposé par Gamow. </figure>
            </p>
            
            <h3>L'univers jeune dominé par les photons</h3>
            <p>En 1948, Gamow, Alpher et Herman publient de nombreux papiers dans le cadre de cette théorie. Ils comprennent que pour expliquer la présence importante d'hydrogène, il est nécessaire que les protons issus de la désintégration des neutrons libres n'aient pas tous formé avec eux du deutéron (noyau constitué d'un proton et d'un neutron). Ils proposent alors que la formation du deutéron soit en fait un équilibre :
            \begin{equation}
            n+p \rightleftharpoons d+\gamma 
            \end{equation}
            Cet équilibre maintient la quantité de neutrons en empêchant leur désintégration (les neutrons libres réagissent pour former du deutéron dans lequel ils sont stables puis sont libérés à nouveau très vite par rapport à leur temps de demi-vie). 
            Pour que la réaction inverse (et donc l'équilibre) soit possible, il faut que l'Univers contienne de l'énergie sous forme de photons à un niveau comparable à l'énergie de dissociation du deutéron, ce qui correspond d'après les trois physiciens à un rayonnement d'une température de l'ordre de ($10^9$ K). Avec l'expansion, l'énergie des photons diminue jusqu'à ce que la réaction inverse soit impossible.  
            Ils comprennent alors que l'Univers devait être très chaud, et que la densité d'énergie de radiation $\sim \sigma T^4/c$ était très supérieure à la densité d'énergie de la matière non relativiste. Ceci a plusieurs implications. D'abord, d'après les équations de Friedmann, cela signifie que l'expansion était gouvernée par le rayonnement. De plus, ce rayonnement qui a du refroidir avec l'expansion devrait toujours exister, et en connaissant sa température à un instant donné (ici celui où les photons cessent de contribuer à l'équilibre du deutéron), il est possible d'en déduire la valeur actuelle. Ce sont Alpher et Herman qui proposent ainsi pour la première fois l'existence de ce qui est aujourd'hui appelé fond diffus cosmologique ("CMB" en anglais pour cosmic microwave background); Ils établissent plusieurs estimations de sa température variant entre quelques Kelvins et quelques dizaines de Kelvins</p>
            <p><!--A la fin des années 1940, deux modèles cosmologiques principaux sont en confrontation : le modèle Einstein-de Sitter-Friedmann-Lemaitre (TODO: ...) dans lequel l'Univers est arbitrairement dense à son origine, et le modèle stationnaire de Hoyle dans lequel l'Univers est toujours semblable. A l'époque, les données issues de l'astronomie ne sont pas encore suffisantes pour déterminer la vraisemblance relative de ces modèles. En revanche, la présence d'une période très dense n'est pas anodine. Plusieurs physiciens s'intéressent donc alors à la dynamique de l'Univers dans cette phase que Hoyle surnomme alors "Big Bang".--></p>
            <p>En 1950, des travaux menés par Enrico Fermi et Anthony Turkevich portant sur les réactions nucléaires entre éléments de taille $A \leq 7$ améliorent de façon significative la nature des processus en jeu dans ce modèle et de leur section efficace. Il apparait alors que ces réactions ne peuvent expliquer la formation d'éléments plus lourds que le béryllium (TODO: $A = 5,8$ posent pb).</p>
            <p>Parallèlement, Hayashi suggère que des mécanismes électrofaibles ont instauré un équilibre qui a imposé le rapport $p/n$ avant la nucléosynthèse, en contradiction avec l'hypothèse initiale de Gamow d'un état initial constitué uniquement de neutrons dont la désintégration serait la seule source de protons.
            \begin{equation}
            p+e^- \rightleftharpoons n+\nu_e
            \end{equation}
            Ainsi, Hayashi trouve un ratio protons/neutrons $n_p/n_n \sim 4$ au lieu de $1/7$ environ comme estimé par Gamow, Alpher et Herman en ne considérant que la désintégration des neutrons. Or, cette valeur ne permet pas de rendre compte de l'abondance observée des éléments pour une nucléosynthèse par capture neutronique successive. 
            
            Puisque la détermination de ce ratio $p/n$ est cruciale pour déterminer la vraisemblance de la nucléosynthèse par capture neutronique, Alpher, Herman et Follin publient un papier en 1953 visant à estimer l'état initial de l'Univers avant la nucléosynthèse, selon les développements théoriques à leur disposition (qui leur permettent de décrire assez précisemment les phénomènes en jeu jusqu'à une température d'environ 100 MeV ~ $10^12$ K) et en étudiant la dépendance en certaines valeurs expérimentales mal connues (par exemple, le temps de demie-vie du neutron). Ils estiment que $p/n$ est compris entre $4,5$ et $6$, ce qui remet en effet en cause la nucléosynthèse par capture neutronique. Ce papier constitue cependant une base importante en tant que description alors la plus détaillée des premiers instants d'un big bang chaud.
            <figure src="1953_bigbang_timetable.png" title="Récapitulatif des différentes étapes du Big Bang selon Alpher-Hermann-Follin 1953" width="300">
                
            </figure>
            </p>
        </text>
    </content>
    
    <content id="8">
        <title>Nucléosynthèse stellaire et nucléosynthèse primordiale</title>
        <image>nucleosynthese_stellaire.jpg</image>
        <text>
            <p>
                Grâce aux travaux d'Alpher, Gamow et Herman, on sait décrire dans les années 1950 un Univers en évolution de type Big Bang dans son jeune âge. On sait que le refroidissement rapide à partir de températures très élevées permet d'expliquer la formation d'éléments légers (jusqu'à $A = 5$), mais pas des éléments plus lourds. On sait par ailleurs qu'il doit subsister, dans ce modèle, une densité de rayonnement non nulle à notre époque, équivalente à un rayonnement de corps noir dont la température actuelle devrait être de l'ordre de grandeur $1-10 K$. Cependant, à l'époque, l'idée de Big Bang demeure assez spéculative et souffre de plusieurs problèmes<note>comsic age problem</note> et la situation en reste là.
            </p>
            <p>
                En 1957, 
            </p>
        </text>
    </content>
    
    <content id="100">
        <title>Evolution de la physique des particules</title>
        <image></image>
        <text>
            <p>Test</p>
            <feynman id="test" title="Test !">
                description: 'Four-gluon vertex for QCD',
                width: 480,
                height: 140,
                incoming: {i1: '40,120', i2: '140,120', i3: '40,20', i4: '140,20'},
                vertex: {v1: '90,70'},
                gluon: {line: 'v1-i1,v1-i2,v1-i3,v1-i4'},
                node: {show: 'v', type: 'dot', fill: 'black', radius: 2},
                label: {t1: ['15,110', '$c,\\rho$', 30], t2: ['140,110', '$d,\\sigma$', 30],
                t3: ['15,10', '$b,\\nu$', 30], t4: ['140,10', '$a,\\mu$', 30],
                t5: ['190,15', '$=-ig^2\\big[\\!f^{abe}f^{cde}(\\eta^{\\mu\\rho}\\eta^{\\nu\\sigma}-\\eta^{\\mu\\sigma}\\eta^{\\nu\\rho})\\\\\\qquad+f^{ace}f^{bde}(\\eta^{\\mu\\nu}\\eta^{\\rho\\sigma}-\\eta^{\\mu\\sigma}\\eta^{\\nu\\rho})\\\\\\qquad+f^{ade}f^{bce}(\\eta^{\\mu\\nu}\\eta^{\\rho\\sigma}-\\eta^{\\mu\\rho}\\eta^{\\nu\\sigma})\\big]$', 290, 100]},
                mathjax: true
            </feynman>
            
            <feynman id="nu_e" title="electron neutrino scattering">
                description: 'Neutrino / electron pair annihilation',
                width: 480,
                height: 220,
                incoming: {i1: '40,20', i2: '40,140'},
                outgoing: {o1: '240,20', o2: '240,140'},
                vertex: {v1: '140,50', v2:'140,110'},
                fermion: {line: 'i2-v2-o2,i1-v1-o1'},
                photon: {line: 'v1-v2'},
                label: {t1: ['50,0', '$\\nu_e$'], t2: ['50,150', '$e^-$'], 
                t3: ['100,80', '$W^-$'],
                t4: ['230,0', '$e^-$'], t5: ['230,150', '$\\nu_e$']},
                mathjax: true
            </feynman>
        </text>
    </content>
</contents>
